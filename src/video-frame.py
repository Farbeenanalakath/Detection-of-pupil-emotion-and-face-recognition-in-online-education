from src.encode_faces import *# Loading the video file# cap = cv2.VideoCapture('vid1.mp4')face_cascade = cv2.CascadeClassifier(r'model/haarcascade_frontalface_default.xml')while True:    con = pymysql.connect(host="localhost", port=3306, user="root", password="Root_root1", database="pupil_emotion")    cmd = con.cursor()    cmd.execute("SELECT * FROM `video` WHERE `id` NOT IN(SELECT `id` FROM `video_status`)")    s=cmd.fetchall()    for r in s:        cmd.execute("insert into video_status values("+str(r[0])+")")        print(r[2],"==========================")        cap = cv2.VideoCapture(r'static/video/'+r[2])        while (True):            ret, img = cap.read()            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)            faces = face_cascade.detectMultiScale(gray, 1.3, 5)            # print(faces) #locations of detected faces            emotion = None            for (x, y, w, h) in faces:                cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)  # draw rectangle to main image                img1=img[int(y):int(y + h), int(x):int(x + w)]                enf(img1,r[0])        cap.release()   # Closing the video stream